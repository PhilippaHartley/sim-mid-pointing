PYTHONPATH is /alaska/tim/Code/algorithm-reference-library/::/alaska/tim/Code/algorithm-reference-library/
Running python: /alaska/tim/alaska-venv/bin/python
Running dask-scheduler: /alaska/tim/alaska-venv/bin/dask-scheduler
Changed directory to /alaska/tim/Code/sim-mid-pointing/arl_simulation/platinum_simulations/case2.

openhpc-compute-[0-9,11-15]
Working on openhpc-compute-0 ....
run dask-scheduler
distributed.scheduler - INFO - -----------------------------------------------
run dask-worker
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.14:33544'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.14:33634'
Working on openhpc-compute-1 ....
run dask-worker
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-yaramct_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-xmrr0jpb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-56rut_a7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-e4ibjjyk', purging
Working on openhpc-compute-2 ....
run dask-worker
Working on openhpc-compute-3 ....
run dask-worker
distributed.scheduler - INFO - Clear task state
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.14:45548
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.14:37872
distributed.worker - INFO -          Listening to:   tcp://10.60.253.14:45548
distributed.worker - INFO -          Listening to:   tcp://10.60.253.14:37872
distributed.worker - INFO -          dashboard at:         10.60.253.14:37119
distributed.worker - INFO -          dashboard at:         10.60.253.14:40786
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-dbrmhlrz
distributed.worker - INFO -       Local Directory:       /tmp/worker-cav4xnuk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.dashboard.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: pip install jupyter-server-proxy
distributed.scheduler - INFO -   Scheduler at:   tcp://10.60.253.14:8786
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.scheduler - INFO - Local Directory:    /tmp/scheduler-eb3eo5uo
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.14:45548
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.14:45548
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.14:37872
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.14:37872
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
Working on openhpc-compute-4 ....
run dask-worker
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.19:33136'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.19:41240'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-a70itqje', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-p20y62pt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-1b4o4s3m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-egmjbrgn', purging
Working on openhpc-compute-5 ....
run dask-worker
ssh_exchange_identification: Connection closed by remote host
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.38:38158'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.38:32990'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.23:35172'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.23:39450'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-fp3kdx_t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-3_fd0762', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-22aih5ni', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-liab29ty', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-3w4zbk7s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-gn1uwxzh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-ad3krj0i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-3hiv2rts', purging
Working on openhpc-compute-6 ....
run dask-worker
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.39:41340'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.39:32965'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-bnhn0qje', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-2hoik1i3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-p_jzjsq6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-u3j4apba', purging
Working on openhpc-compute-7 ....
run dask-worker
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.18:36574'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.18:40878'
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.23:40655
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.23:46719
distributed.worker - INFO -          Listening to:   tcp://10.60.253.23:40655
distributed.worker - INFO -          Listening to:   tcp://10.60.253.23:46719
distributed.worker - INFO -          dashboard at:         10.60.253.23:46159
distributed.worker - INFO -          dashboard at:         10.60.253.23:45786
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-xjg_v6nh
distributed.worker - INFO -       Local Directory:       /tmp/worker-b2bfoudv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.23:40655
distributed.scheduler - INFO - Register tcp://10.60.253.23:46719
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.23:40655
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.23:46719
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
Working on openhpc-compute-8 ....
run dask-worker
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-2ttilcin', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-3xmzm28a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-29tuf56m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-4r7pdkkp', purging
Working on openhpc-compute-9 ....
run dask-worker
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.39:33476
distributed.worker - INFO -          Listening to:   tcp://10.60.253.39:33476
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.39:45769
distributed.worker - INFO -          dashboard at:         10.60.253.39:35022
distributed.worker - INFO -          Listening to:   tcp://10.60.253.39:45769
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -          dashboard at:         10.60.253.39:36769
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-svuqaybo
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-eza0ti1k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.39:45769
distributed.scheduler - INFO - Register tcp://10.60.253.39:33476
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.39:45769
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.39:33476
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.19:33881
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.19:45121
distributed.worker - INFO -          Listening to:   tcp://10.60.253.19:33881
distributed.worker - INFO -          Listening to:   tcp://10.60.253.19:45121
distributed.worker - INFO -          dashboard at:         10.60.253.19:37219
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -          dashboard at:         10.60.253.19:33283
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-dxt0naw3
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory:       /tmp/worker-evtavdxg
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.19:45121
distributed.scheduler - INFO - Register tcp://10.60.253.19:33881
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.19:45121
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.19:33881
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.41:37364'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.41:45804'
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.38:42036
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.38:33958
distributed.worker - INFO -          Listening to:   tcp://10.60.253.38:33958
distributed.worker - INFO -          dashboard at:         10.60.253.38:42624
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -          Listening to:   tcp://10.60.253.38:42036
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         10.60.253.38:44113
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-ceg3gu0d
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-yyo66vdd
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.38:33958
distributed.scheduler - INFO - Register tcp://10.60.253.38:42036
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.38:33958
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.38:42036
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
Working on openhpc-compute-11 ....
run dask-worker
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-y8_azn3j', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-7yfqq6h2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-ajmv1oio', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-frbvs2uv', purging
Working on openhpc-compute-12 ....
run dask-worker
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.18:41533
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.18:41575
distributed.worker - INFO -          Listening to:   tcp://10.60.253.18:41533
distributed.worker - INFO -          dashboard at:         10.60.253.18:37608
distributed.worker - INFO -          Listening to:   tcp://10.60.253.18:41575
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -          dashboard at:         10.60.253.18:39936
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory:       /tmp/worker-mip7oc8y
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory:       /tmp/worker-3ziek_d6
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.18:41533
distributed.scheduler - INFO - Register tcp://10.60.253.18:41575
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.18:41533
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.18:41575
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.55:42929'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.55:38687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.33:46698'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.33:33985'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-ng9equxi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-m3kifwkl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-3onp5f78', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-e4ol839h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-27f2uljt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-nvxuagj3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-2ahwkp9b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-ux3zxhyp', purging
Working on openhpc-compute-13 ....
run dask-worker
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.36:43280'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.36:44160'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-cqiz1k0w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-cx6pnxoi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-veij3wn9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-8qo9kexo', purging
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.41:40138
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.41:42198
distributed.worker - INFO -          Listening to:   tcp://10.60.253.41:40138
distributed.worker - INFO -          Listening to:   tcp://10.60.253.41:42198
distributed.worker - INFO -          dashboard at:         10.60.253.41:35532
distributed.worker - INFO -          dashboard at:         10.60.253.41:45817
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-ro5u9ifa
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-oe2bpwe1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.41:40138
distributed.scheduler - INFO - Register tcp://10.60.253.41:42198
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.41:40138
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.41:42198
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.51:34212'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.51:35122'
Working on openhpc-compute-14 ....
run dask-worker
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-p85f3832', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-y1zugvqy', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-nf8j2dmr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-gunm38_s', purging
Working on openhpc-compute-15 ....
run dask-worker
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.36:40890
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.36:35191
distributed.worker - INFO -          Listening to:   tcp://10.60.253.36:40890
distributed.worker - INFO -          Listening to:   tcp://10.60.253.36:35191
distributed.worker - INFO -          dashboard at:         10.60.253.36:43132
distributed.worker - INFO -          dashboard at:         10.60.253.36:46507
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-65s99z96
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory:       /tmp/worker-ovhi882m
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.36:35191
distributed.scheduler - INFO - Register tcp://10.60.253.36:40890
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.36:35191
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.36:40890
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
Scheduler and workers now running
Scheduler is running at openhpc-compute-0
About to execute python ../../pointing_simulation_distributed.py --context s3sky --rmax 1e5 --flux_limit 0.003 --ngroup_visibility 24 --ngroup_components 100 --show True --seed 18051955  --pbtype MID_GRASP --memory 32  --integration_time 30 --use_agg True --static_pe 0.1 0.01 --dynamic 0.0 --time_chunk 1800 --shared_directory ../../../shared | tee pointing_simulation.log
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.37:34613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.37:33811'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.12:46764'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.12:44198'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-qs3nnqu4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-umrt4otx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-irylffk9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-vxegjkn3', purging
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.51:33225
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.51:36578
distributed.worker - INFO -          Listening to:   tcp://10.60.253.51:33225
distributed.worker - INFO -          dashboard at:         10.60.253.51:45739
distributed.worker - INFO -          Listening to:   tcp://10.60.253.51:36578
distributed.worker - INFO -          dashboard at:         10.60.253.51:44260
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-frvcep4i
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-7k5utwd7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.51:33225
distributed.scheduler - INFO - Register tcp://10.60.253.51:36578
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.51:33225
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.51:36578
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-1fq6rxdt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-g0lu2l8o', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-1_mqctz5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-_bqktjzf', purging
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.55:33217
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.55:39526
distributed.worker - INFO -          Listening to:   tcp://10.60.253.55:39526
distributed.worker - INFO -          Listening to:   tcp://10.60.253.55:33217
distributed.worker - INFO -          dashboard at:         10.60.253.55:42689
distributed.worker - INFO -          dashboard at:         10.60.253.55:44636
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-469dpv6q
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-o66q7_ic
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.55:39526
distributed.scheduler - INFO - Register tcp://10.60.253.55:33217
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.55:39526
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.55:33217
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.33:43925
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.33:38430
distributed.worker - INFO -          Listening to:   tcp://10.60.253.33:38430
distributed.worker - INFO -          Listening to:   tcp://10.60.253.33:43925
distributed.worker - INFO -          dashboard at:         10.60.253.33:41836
distributed.worker - INFO -          dashboard at:         10.60.253.33:42465
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-sii8v1vu
distributed.worker - INFO -       Local Directory:       /tmp/worker-2ald0qaq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.33:43925
distributed.scheduler - INFO - Register tcp://10.60.253.33:38430
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.33:43925
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.33:38430
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.13:43577'
distributed.nanny - INFO -         Start Nanny at: 'tcp://10.60.253.13:32792'
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-jjm7kapo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-wimmp8dd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-z9o3gxgb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/tmp/worker-u20hznxx', purging
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.37:34521
distributed.worker - INFO -          Listening to:   tcp://10.60.253.37:34521
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.37:33550
distributed.worker - INFO -          Listening to:   tcp://10.60.253.37:33550
distributed.worker - INFO -          dashboard at:         10.60.253.37:39914
distributed.worker - INFO -          dashboard at:         10.60.253.37:36857
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-xgz76b0m
distributed.worker - INFO -       Local Directory:       /tmp/worker-yds52g9b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.37:34521
distributed.scheduler - INFO - Register tcp://10.60.253.37:33550
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.37:34521
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.37:33550
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.12:36083
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.12:45297
distributed.worker - INFO -          Listening to:   tcp://10.60.253.12:36083
distributed.worker - INFO -          dashboard at:         10.60.253.12:44184
distributed.worker - INFO -          Listening to:   tcp://10.60.253.12:45297
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO -          dashboard at:         10.60.253.12:39027
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-v7iwl2wt
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-kawi5_of
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.12:36083
distributed.scheduler - INFO - Register tcp://10.60.253.12:45297
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.12:36083
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.12:45297
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.13:44115
distributed.worker - INFO -       Start worker at:   tcp://10.60.253.13:33463
distributed.worker - INFO -          Listening to:   tcp://10.60.253.13:44115
distributed.worker - INFO -          Listening to:   tcp://10.60.253.13:33463
distributed.worker - INFO -          dashboard at:         10.60.253.13:46346
distributed.worker - INFO -          dashboard at:         10.60.253.13:33129
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - Waiting to connect to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -                Memory:                  101.18 GB
distributed.worker - INFO -       Local Directory:       /tmp/worker-rdw58bv8
distributed.worker - INFO -       Local Directory:       /tmp/worker-63dvfgjp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register tcp://10.60.253.13:33463
distributed.scheduler - INFO - Register tcp://10.60.253.13:44115
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.13:33463
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.13:44115
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://openhpc-compute-0:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-b4f50458-aa0a-11e9-8e83-246e964883a8
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.scheduler - INFO - Register tcp://10.60.253.14:41233
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.14:41233
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.14:34679
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.14:34679
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.19:40432
distributed.scheduler - INFO - Register tcp://10.60.253.19:39006
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.19:40432
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.19:39006
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.38:44415
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.38:44415
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.38:38950
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.38:38950
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.23:41747
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.23:41747
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.23:38918
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.23:38918
distributed.core - INFO - Starting established connection
 
Distributed simulation of pointing errors for SKA-MID
-----------------------------------------------------
 
Random number seed is 18051955
Creating Dask Client using externally defined scheduler
Start times for chunks:
array([-21600., -19800., -18000., -16200., -14400., -12600., -10800.,
        -9000.,  -7200.,  -5400.,  -3600.,  -1800.,      0.,   1800.,
         3600.,   5400.,   7200.,   9000.,  10800.,  12600.,  14400.,
        16200.,  18000.,  19800.])
Observation times:
1440 integrations of duration 30.0 s processed in 24 chunks
MID_GRASP: HWHM beam = 0.751 deg
Constructing s3sky components
create_test_skycomponents_from_s3: Reading S3-SEX sources from /alaska/tim/Code/algorithm-reference-library/data/models/S3_1400MHz_100uJy_18deg.csv 
distributed.scheduler - INFO - Register tcp://10.60.253.39:42721
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.39:42721
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.39:40406
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.39:40406
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.18:44064
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.18:44064
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.18:35877
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.18:35877
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.41:44903
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.41:44903
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.41:38434
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.41:38434
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.33:35360
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.33:35360
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.33:46479
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.33:46479
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.55:33683
distributed.scheduler - INFO - Register tcp://10.60.253.55:37352
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.55:33683
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.55:37352
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.36:36738
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.36:36738
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.36:33090
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.36:33090
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.51:37568
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.51:37568
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.51:34664
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.51:34664
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.12:40105
distributed.scheduler - INFO - Register tcp://10.60.253.12:41960
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.12:40105
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.12:41960
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.37:33292
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.37:33292
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.37:37055
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.37:37055
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.13:40648
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.13:40648
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register tcp://10.60.253.13:36523
distributed.scheduler - INFO - Starting worker compute stream, tcp://10.60.253.13:36523
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-0396e96c-aa0b-11e9-902f-246e964883a8
distributed.core - INFO - Starting established connection
create_test_skycomponents_from_s3: 5491 sources found above fluxlimit inside search radius
distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 1.39 GB from 2685 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 409.19 MB from 396 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 130.31 MB from 387 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 130.18 MB from 1395 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 130.37 MB from 1450 reference cycles (threshold: 10.00 MB)
distributed.utils_perf - INFO - full garbage collection released 130.42 MB from 155 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 6.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
